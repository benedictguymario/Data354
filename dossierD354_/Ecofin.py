#CHARGEMENT DES BIBLIOTHEQUES
from typing import List, Dict
import chainlit as cl
from langchain.vectorstores import Chroma
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import os
from dotenv import load_dotenv

# Chargement  les variables d'environnement
load_dotenv()


# Configuration de l'API GenAI
API_KEY = os.getenv("GENAI_API_KEY")
genai.configure(api_key=API_KEY)

# Mod√®le d'embedding
class SentenceTransformerEmbeddings:
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self.model.encode(texts, convert_to_tensor=True).tolist()

    def embed_question(self, text: str) -> List[float]:
        return self.model.encode(text, convert_to_tensor=True).tolist()

# Cr√©ation du prompt avec historique
def Creat_prompt(question: str, reponse: str, historique: List[Dict[str, str]]) -> str:
    #  l'historique de la conversation
    historique_str = "\n".join([f"Utilisateur : {h['question']}\nAssistant : {h['response']}" for h in historique])
    

    prompt=f"""
        Vous √™tes un assistant expert charg√© de r√©pondre aux questions des utilisateurs de mani√®re claire, d√©taill√©e et pr√©cise. Voici les instructions √† suivre :
        0. **salutation**: r√©pond aux salutations simplement
        1. **Langue**: Si la question est autre que le fran√ßais R√©pondez toujours dans la m√™me langue que la question de l'utilisateur.
        2. **Contexte**: Utilisez les √©l√©ments de contexte suivants pour r√©pondre √† la question de l'utilisateur. Si le contexte est insuffisant, indiquez-le clairement et fournissez une r√©ponse g√©n√©rale ou sugg√©rez des pistes de r√©flexion.
        3. **Historique**: Voici l'historique de la conversation :
           {historique_str}
        4. **Format**: pour une meilleure lisibilit√©. Utilisez des titres, des listes et des mises en forme pour structurer la r√©ponse.
        5. **D√©tails**: Si la r√©ponse est g√©n√©rale ou incertaine, signalez-le clairement. Si la question est vague ou trop large, demandez des pr√©cisions √† l'utilisateur.
        6. **Suggestions**: Si la r√©ponse est insuffisante ou absente, sugg√©rez √† l'utilisateur des questions li√©es, comme :
           - ¬´ Souhaitez-vous en savoir plus sur un autre sujet ? ¬ª
           - ¬´ Peut-√™tre pouvez-vous poser une question plus pr√©cise sur ce sujet ? ¬ª
           - ¬´ Si vous avez des questions sur un autre sujet, n'h√©sitez pas √† les poser. ¬ª
        7. **D√©veloppement**: Si la r√©ponse de l'utilisateur est une affirmation, d√©veloppez davantage la r√©ponse en fournissant plus de d√©tails.
        8. **Questions courtes**: Si la question est un seul mot, demandez √† l'utilisateur s'il souhaite des informations g√©n√©rales ou une r√©ponse pr√©cise. Dans ce cas, ne retournez pas de ¬´ source ¬ª.
        9. **Emojis**: Ajoutez des emojis pour rendre la r√©ponse plus engageante et amicale.

        ---

        **Contexte**: {reponse}

        **Question**: {question}

        ---

        **R√©ponse utile**:
        """
    return prompt

# Chargement de la base de donn√©es
def Obtenir_db(chroma_db_path: str, fonc_embed: SentenceTransformerEmbeddings):
    try:
        db = Chroma(persist_directory=chroma_db_path, embedding_function=fonc_embed)
    except Exception as e:
        return None, f"Erreur : Impossible d'acc√©der √† la collection. D√©tail : {str(e)}"
    return db, None

# Recherche de contexte dans la base
def Obtenir_contexte(db, question: str, fonc_embed: SentenceTransformerEmbeddings, k: int = 3) -> str:
    try:
        question_embedding = fonc_embed.embed_question(question)
        results = db.similarity_search_by_vector(question_embedding, k=k)
        if not results:
            return "Aucun contexte trouv√©."
        docs = [result.page_content for result in results]
        contexte = "\n\n".join(docs)  # Combiner plusieurs articles
    except Exception as e:
        return f"Erreur lors de la r√©cup√©ration du contexte : {str(e)}"
    return contexte

# G√©n√©ration de r√©ponse
def Reponse(chatbot, prompt: str,model: str) -> str:
    try:
        if model == "gemini-2.0-flash-thinking-exp":
            # Ajouter la partie raisonnement
            thinking_prompt = f"""
            **D√©marche de r√©flexion :**
            Avant de r√©pondre, veuillez analyser et expliquer chaque √©tape de votre raisonnement pour cette question.
            Ensuite, fournissez la r√©ponse d√©taill√©e.

            **Question :** {prompt}
            """
            response = chatbot.generate_content(thinking_prompt)  # G√©n√©rer une r√©ponse en mode raisonnement
            final_response = response.text # R√©cup√©rer la r√©ponse g√©n√©r√©e
        else:
            # Utiliser le mode normal pour les autres mod√®les
            response = chatbot.generate_content(prompt)
            final_response = response.text
    except Exception as e:
        return f"Erreur lors de la g√©n√©ration de la r√©ponse : {str(e)}"
    return final_response

# Gestionnaire d'√©v√©nements Chainlit
@cl.on_chat_start
async def chat_start():
    # Charger le mod√®le d'embedding et la base de donn√©es une seule fois
    fonc_embed = SentenceTransformerEmbeddings()
    db, error = Obtenir_db("Chromadb", fonc_embed)
    if error:
        await cl.Message(content=f"Erreur lors du chargement de la base de donn√©es : {error}").send()
        return
    cl.user_session.set("db", db)
    cl.user_session.set("fonc_embed", fonc_embed)
    cl.user_session.set("historique", [])  # Initialiser l'historique de la conversation
    await cl.Message(
        "üëã Bienvenue ! Je suis Ecofin, votre assistant sp√©cialis√© pour les articles d'Ecofine.\n"
        "üí° Vous pouvez activer le mode raisonnement pour des analyses plus approfondies.\n"
        "De quel sujet souhaitez-vous discuter ?" 
        ).send()

@cl.set_chat_profiles
# fenetre de choix des model
async def chat_profile():
    return [
        cl.ChatProfile(
            name="gemini-2.0-flash-exp",
            markdown_description="üí® Le mod√®le **gemini-2.0-flash-exp**.",
            icon="https://picsum.photos/200",
        ),
        cl.ChatProfile(
            name="gemini-2.0-flash-thinking-exp",
            markdown_description="ü§î Mode raisonnement.",
            icon="https://picsum.photos/250",
        ),
        cl.ChatProfile(
            name="gemini-1.5-flash",
            markdown_description="‚ö° Le mod√®le **gemini-1.5-flash**.",
            icon="https://picsum.photos/300",
        ),
    ]

@cl.on_message
async def on_message(message: cl.Message):
    db = cl.user_session.get("db")
    fonc_embed = cl.user_session.get("fonc_embed")
    historique = cl.user_session.get("historique")  # R√©cup√©ration l'historique de la conversation
    if not db or not fonc_embed:
        await cl.Message(content="Erreur : Base de donn√©es non disponible.").send()
        return

    question = message.content
    contexte = Obtenir_contexte(db, question, fonc_embed, k=3)  # R√©cup√©rer 3 articles pertinents
    prompt = Creat_prompt(question, contexte, historique)  # Inclusion de l'historique dans le prompt
    model = cl.user_session.get("chat_profile")
    response = Reponse(genai.GenerativeModel(model), prompt,model)
    await cl.Message(content=response).send()

    # Mise √† jour l'historique de la conversation
    historique.append({"question": question, "response": response})
    cl.user_session.set("historique", historique)

    # Demander si l'utilisateur est satisfait
    res = await cl.AskActionMessage(
        content="Etes vous satisfait de la reponse ?",
        actions=[
            cl.Action(name="Merci pour votre remarque", payload={"value": "continue"}, label="üëçOUI"),
            cl.Action(name="Desole prochainement je essayer de faire mieux", payload={"value": "cancel"}, label="üëé NON"),
        ],
    ).send()

    if res and res.get("payload").get("value") == "continue":
        await cl.Message(
            content="Merci pour votre remarqueüòä",
        ).send()
    else:
        await cl.Message(
            content="Desole prochainement je essayer de faire mieux üôè ",
        ).send()

if __name__ == "__main__":
    cl.run()
   